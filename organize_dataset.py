import os
import shutil
import pandas as pd
from sklearn.model_selection import train_test_split

# --- ç”¨æˆ·é…ç½® ---

# 1. å®šä¹‰æˆ‘ä»¬æ‰€æœ‰ä¸‹è½½çš„ã€åŸå§‹çš„æ•°æ®æº
#    è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæè¿°äº†æ•°æ®æºçš„ä¿¡æ¯
SOURCE_DATASETS = [
    {
        "name": "Roboflow_v1",
        "path": "Mydataset/Roboflow_v1",
        "type": "roboflow_classification" # æˆ‘ä»¬å®šä¹‰ä¸€ç§ç±»å‹å« 'roboflow_classification'
    },
    {
        "name": "Roboflow_v2",
        "path": "Mydataset/roboflow_v2",
        "type": "roboflow_classification"
    },
    {
        "name": "Roboflow_v3",
        "path": "Mydataset/roboflow_v3",
        "type": "roboflow_classification"
    },
    {
        "name": "Kaggle_Pepper",
        "path": "Mydataset/kaggle_black_pepper_dataset",
        "type": "folder_per_class" # å¦ä¸€ç§ç±»å‹ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹ä»£è¡¨ä¸€ä¸ªç±»åˆ«
    },
    {
        "name": "Mendeley_Pepper",
        "path": "Mydataset/Pepper Diseases and Pests Detection",
        "type": "manual_sort" # å®šä¹‰ä¸€ç§æ–°ç±»å‹ï¼Œæé†’è‡ªå·±è¿™ä¸ªéœ€è¦æ‰‹åŠ¨å¤„ç†
    },
]

# 2. å®šä¹‰æœ€ç»ˆçš„ç›®æ ‡â€œå…µå·¥å‚â€æ–‡ä»¶å¤¹
MASTER_DATASET_DIR = "Masterdataset"

# 3. å®šä¹‰ä½ çš„ç±»åˆ«æ˜ å°„
#    è¿™é‡Œçš„Keyï¼Œå¿…é¡»å’ŒåŸå§‹æ•°æ®é›†é‡Œçš„ç±»åˆ«åå®Œå…¨ä¸€è‡´ï¼
#    Value æ˜¯ä½ å¸Œæœ›åœ¨ MasterDataset é‡Œåˆ›å»ºçš„ã€ç»Ÿä¸€çš„æ–‡ä»¶å¤¹åã€‚
CLASS_MAPPING = {
    # ç¤ºä¾‹: ä½ éœ€è¦æ ¹æ®ä½ â€œä¾¦å¯Ÿâ€åˆ°çš„çœŸå®ç±»åˆ«åæ¥å¡«å†™
    # Roboflow (éœ€è¦ä½ æ‰“å¼€ _annotations.csv æˆ– _classes.csv æŸ¥çœ‹)
    "black_pepper": "Pepper_Black", 
    "white_pepper": "Pepper_White",
    "Bacterial-Spot": "Pepper_Bacterial_Spot", # Roboflow å¯èƒ½ä½¿ç”¨ '-'
    
    # Kaggle
    "black_pepper_healthy": "Pepper_Healthy",
    "Leaf_blight": "Pepper_Leaf_Blight",
    "Yallow_Mottle_virus": "Pepper_Yellow_Mottle_Virus",
}

def organize_roboflow_dataset(source_path, master_path, mapping):
    """å¤„ç† Roboflow å¯¼å‡ºæ ¼å¼çš„æ•°æ®é›† (æ™ºèƒ½å¯»æ‰¾æ ‡æ³¨æ–‡ä»¶å¹¶å®¹é”™)"""
    print(f"\n--- æ­£åœ¨å¤„ç† Roboflow æ•°æ®é›†: {source_path} ---")
    for subset in ["train", "valid", "test"]:
        subset_path = os.path.join(source_path, subset)
        
        # --- æ™ºèƒ½å¯»è·¯é€»è¾‘ ---
        annotation_file_v1 = os.path.join(subset_path, "_annotations.csv")
        annotation_file_v2 = os.path.join(subset_path, "_classes.csv")
        
        annotation_file = None
        if os.path.exists(annotation_file_v1):
            annotation_file = annotation_file_v1
        elif os.path.exists(annotation_file_v2):
            annotation_file = annotation_file_v2
        
        # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•æ ‡æ³¨æ–‡ä»¶
        if not annotation_file:
            print(f"âš ï¸  åœ¨ '{subset_path}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•æ ‡æ³¨æ–‡ä»¶ï¼Œå°†å°è¯•ä½œä¸ºæœªåˆ†ç±»æ•°æ®å¤„ç†...")
            # --- å®¹é”™é€»è¾‘ï¼šå¤„ç†æ²¡æœ‰æ ‡æ³¨æ–‡ä»¶çš„æ–‡ä»¶å¤¹ ---
            unlabeled_target_path = os.path.join(master_path, 'train', '_unlabeled') # å…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†çš„'æœªæ ‡æ³¨'æ–‡ä»¶å¤¹
            os.makedirs(unlabeled_target_path, exist_ok=True)
            
            if os.path.isdir(subset_path):
                image_count = 0
                for f in os.listdir(subset_path):
                    if f.lower().endswith(('.png', '.jpg', '.jpeg')):
                        # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„æ–‡ä»¶åä»¥é¿å…å†²çª
                        unique_filename = f"unlabeled_{dataset_info['name']}_{subset}_{f}"
                        shutil.copy(os.path.join(subset_path, f), os.path.join(unlabeled_target_path, unique_filename))
                        image_count += 1
                if image_count > 0:
                    print(f"   å·²å°† {image_count} å¼ æœªæ ‡æ³¨å›¾ç‰‡å¤åˆ¶åˆ° 'train/_unlabeled' æ–‡ä»¶å¤¹ç­‰å¾…æ‰‹åŠ¨åˆ†ç±»ã€‚")
            continue # å¤„ç†ä¸‹ä¸€ä¸ªå­é›† (valid, test)
            
        # å¦‚æœæ‰¾åˆ°äº†æ ‡æ³¨æ–‡ä»¶
        print(f"ğŸ“‚ æ­£åœ¨ä½¿ç”¨ '{os.path.basename(annotation_file)}' æ•´ç† '{subset}' å­é›†...")
        try:
            # å¥å£®åœ°è¯»å–CSVï¼Œæ— è®ºæ˜¯å¦æœ‰è¡¨å¤´
            try:
                df = pd.read_csv(annotation_file)
                if 'filename' in df.columns and 'class' in df.columns:
                     filename_col, classname_col = 'filename', 'class'
                else:
                     df = pd.read_csv(annotation_file, header=None)
                     filename_col, classname_col = 0, 1
            except Exception:
                 df = pd.read_csv(annotation_file, header=None)
                 filename_col, classname_col = 0, 1

            for _, row in df.iterrows():
                image_filename = str(row[filename_col]) # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                class_name_raw = str(row[classname_col]).strip()
                
                target_folder_name = mapping.get(class_name_raw)
                if not target_folder_name: continue

                source_image_path = os.path.join(subset_path, image_filename)
                master_subset_path = os.path.join(master_path, subset, target_folder_name)
                os.makedirs(master_subset_path, exist_ok=True)
                destination_image_path = os.path.join(master_subset_path, os.path.basename(image_filename))
                
                if os.path.exists(source_image_path):
                    shutil.copy(source_image_path, destination_image_path)
        except Exception as e:
            print(f"   âŒ å¤„ç†æ ‡æ³¨æ–‡ä»¶ '{annotation_file}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def organize_folder_per_class_dataset(source_path, master_path, mapping):
    """å¤„ç†æ¯ä¸ªæ–‡ä»¶å¤¹ä»£è¡¨ä¸€ä¸ªç±»åˆ«çš„æ•°æ®é›† (å¦‚Kaggle)"""
    print(f"\n--- æ­£åœ¨å¤„ç† Folder-per-class æ•°æ®é›†: {source_path} ---")
    
    for class_folder_raw in os.listdir(source_path):
        target_folder_name = mapping.get(class_folder_raw)
        if not target_folder_name: continue
        
        source_class_path = os.path.join(source_path, class_folder_raw)
        if not os.path.isdir(source_class_path): continue

        print(f"ğŸ“‚ æ­£åœ¨æ•´ç† '{class_folder_raw}' ç±»åˆ«...")
        
        images = [f for f in os.listdir(source_class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        if not images: continue
        
        # å°†è¿™ä¸ªç±»åˆ«çš„æ•°æ®æŒ‰ 80/20 åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_images, val_images = train_test_split(images, test_size=0.2, random_state=42)
        
        # å¤åˆ¶è®­ç»ƒé›†å›¾ç‰‡
        train_target_path = os.path.join(master_path, 'train', target_folder_name)
        os.makedirs(train_target_path, exist_ok=True)
        for img in train_images:
            shutil.copy(os.path.join(source_class_path, img), os.path.join(train_target_path, img))
            
        # å¤åˆ¶éªŒè¯é›†å›¾ç‰‡
        val_target_path = os.path.join(master_path, 'valid', target_folder_name)
        os.makedirs(val_target_path, exist_ok=True)
        for img in val_images:
            shutil.copy(os.path.join(source_class_path, img), os.path.join(val_target_path, img))
            
def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ‰€æœ‰æ•°æ®é›†çš„æ•´ç†å·¥ä½œ"""
    if os.path.exists(MASTER_DATASET_DIR):
        print(f"è­¦å‘Š: ç›®æ ‡æ–‡ä»¶å¤¹ '{MASTER_DATASET_DIR}' å·²å­˜åœ¨ã€‚è„šæœ¬ä¼šå‘å…¶ä¸­æ·»åŠ æ–‡ä»¶ã€‚")
    os.makedirs(MASTER_DATASET_DIR, exist_ok=True)
    
    global dataset_info # å…è®¸åœ¨ organize_roboflow_dataset ä¸­è®¿é—®
    for dataset_info in SOURCE_DATASETS:
        source_path = dataset_info["path"]
        dataset_type = dataset_info["type"]
        
        if not os.path.isdir(source_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æºæ•°æ®è·¯å¾„ '{source_path}'ï¼Œå·²è·³è¿‡ã€‚")
            continue
            
        if dataset_type == "roboflow_classification":
            organize_roboflow_dataset(source_path, MASTER_DATASET_DIR, CLASS_MAPPING)
        elif dataset_type == "folder_per_class":
            organize_folder_per_class_dataset(source_path, MASTER_DATASET_DIR, CLASS_MAPPING)
        elif dataset_type == "manual_sort":
            print(f"\n--- æç¤º: æ•°æ®é›† '{dataset_info['name']}' éœ€è¦æ‚¨æ‰‹åŠ¨åˆ†ç±»ã€‚---")
            print(f"   è¯·å°† '{dataset_info['path']}' é‡Œçš„å›¾ç‰‡ï¼Œæ‰‹åŠ¨å¤åˆ¶åˆ° '{MASTER_DATASET_DIR}' å¯¹åº”çš„ train/valid ç±»åˆ«æ–‡ä»¶å¤¹ä¸­ã€‚")
        else:
            print(f"æœªçŸ¥çš„æ•°æ®é›†ç±»å‹: {dataset_type}")
            
    print("\n--- âœ… æ‰€æœ‰æ•°æ®é›†æ•´ç†å®Œæˆï¼ ---")
    print(f"è¯·æ£€æŸ¥ '{MASTER_DATASET_DIR}' æ–‡ä»¶å¤¹çš„ç»“æ„å’Œå†…å®¹ï¼Œç‰¹åˆ«æ˜¯ 'train/_unlabeled' æ–‡ä»¶å¤¹ã€‚")

if __name__ == "__main__":
    # åœ¨è¿è¡Œå‰ï¼Œè¯·åŠ¡å¿…å†æ¬¡æ£€æŸ¥å¹¶ä¿®æ”¹ä¸Šé¢çš„ SOURCE_DATASETS å’Œ CLASS_MAPPING
    main()